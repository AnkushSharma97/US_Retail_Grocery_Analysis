{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/fai10105/Project/Data_sets/output already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-585c2bab25d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m#use it for visualization part. However it is not a recommended step as repartition is a costly process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mfully_combined_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/fai10105/Project/Data_sets/output already exists."
     ]
    }
   ],
   "source": [
    "#setting up environement\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/share/spark-3.1.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "#defining sparksession\n",
    "spark = SparkSession.builder.appName('SparkSQL_UseCase').master('local[*]').getOrCreate()\n",
    "\n",
    "\n",
    "#Specifying File Schemas (i.e. Mentioning column name, datatype and Null value status of each column):-\n",
    "\n",
    "#1 Aisles Schema:-\n",
    "aisles_schema= StructType([StructField('aisle_id',IntegerType(),False),\n",
    "                          StructField('aisle',StringType(),True)])\n",
    "\n",
    "#2 Departments_schema:-\n",
    "department_schema=StructType([StructField('department_id',IntegerType(),False),\n",
    "                                StructField('department',StringType(),True)])\n",
    "#3 order_schema:-\n",
    "orders_schema=StructType([StructField('order_id',IntegerType(),False),\n",
    "                              StructField('user_id',IntegerType(),True),\n",
    "                              StructField('eval_set',StringType(),True),\n",
    "                              StructField('order_number',IntegerType(),True),\n",
    "                              StructField('order_dow',IntegerType(),True),\n",
    "                              StructField('order_hour_of_day',IntegerType(),True),\n",
    "                              StructField('days_since_prior_order',IntegerType(),True)])\n",
    "\n",
    "#4 prior_order_schema and train_order_schema:-\n",
    "prior_order_schema=StructType([StructField('order_id',IntegerType(),True),\n",
    "                              StructField('product_id',IntegerType(),True),\n",
    "                              StructField('add_to_cart_order',IntegerType(),True),\n",
    "                              StructField('reordered',IntegerType(),True)])\n",
    "#5 Products_schema:-\n",
    "products_schema=StructType([StructField('product_id',IntegerType(),False),\n",
    "                              StructField('product_name',StringType(),True),\n",
    "                              StructField('aisle_id',StringType(),True),\n",
    "                              StructField('department_id',StringType(),True)])\n",
    "\n",
    "# A) Extracting Data:-\n",
    "#defining file path from where to read the files and output path\n",
    "#Note:- The data has been copied to local and then given the local path here as I was facing issues with Insofe cluster for my IP.\n",
    "dataset_path='/home/fai10105/Project/Data_sets/'\n",
    "output_path=dataset_path+\"/output/\"\n",
    "\n",
    "#reading files as dataframes:-\n",
    "\n",
    "#aisles\n",
    "aisles_df = spark.read\\\n",
    "        .schema(aisles_schema)\\\n",
    "        .option(\"delimeter\",\",\").option(\"header\",\"True\")\\\n",
    "        .csv(dataset_path+'aisles.csv')\n",
    "\n",
    "#departments:-\n",
    "department_df = spark.read\\\n",
    "                .schema(department_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'departments.csv')\n",
    "#orders:-\n",
    "orders_df = spark.read\\\n",
    "                .schema(orders_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'orders.csv')\n",
    "\n",
    "#prior_order:-\n",
    "prior_order_df = spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'prior_order.csv')\n",
    "\n",
    "#products:- reading products file as rdd as it has some noises later on it has been converted to data frame after removing noises. \n",
    "#All other files have been read as csv\n",
    "products_rdd = spark.sparkContext\\\n",
    "                .textFile(dataset_path+'products.csv')\n",
    "\n",
    "#train_order:-\n",
    "train_order_df= spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'train_order.csv')\n",
    "\n",
    "\n",
    "#B) Transformation:- Data Processing Part\n",
    "\n",
    "#removing noises from products data:- removing unwanted characters from records like:- '\\' , '\"' , ',' etc\n",
    "def remove_noise(row):\n",
    "    if '\"' in row:\n",
    "        first=row.index('\"')\n",
    "        last=row.index('\"',first+1)\n",
    "        part_a=row[0:first]\n",
    "        part_b=row[first:last+1].replace(\", \",\" - \").replace('\"','')\n",
    "        part_c=row[last+1:]\n",
    "        row=(part_a+part_b+part_c).replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "    else:\n",
    "        row = row.replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "\n",
    "header=products_rdd.first()\n",
    "products_rdd_mo=products_rdd.filter(lambda x : x!=header).map(lambda x : remove_noise(x))\n",
    "products_df=products_rdd_mo.toDF(products_schema) # product dataframe creation from product rdd after removing noises.\n",
    "\n",
    "\n",
    "#Creating Tables from dataframes for aggregation purposes:-\n",
    "aisles_df.createOrReplaceTempView('aisles') # aisles table\n",
    "department_df.createOrReplaceTempView('department') # department table\n",
    "orders_df.createOrReplaceTempView('orders') # orders table\n",
    "prior_order_df.createOrReplaceTempView('prior_order') #prior_order table\n",
    "products_df.createOrReplaceTempView('products') #products table\n",
    "train_order_df.createOrReplaceTempView('train_order') #train_order table\n",
    "\n",
    "#aggregating products, prior_order and train_order data first (just to make the process easy abd simple)\n",
    "aggregated_table_part_1 =spark.sql('''SELECT p.product_id, product_name, aisle_id, department_id, order_id, add_to_cart_order, reordered\n",
    "                                      FROM products p INNER JOIN train_order to ON to.product_id=p.product_id\n",
    "                                      UNION ALL\n",
    "                                      SELECT p.product_id, product_name, aisle_id, department_id, order_id,add_to_cart_order,reordered\n",
    "                                      FROM products p INNER JOIN prior_order po ON po.product_id=p.product_id''')\n",
    "\n",
    "#creating table from aggregated_table_part_1 dataframe for further aggregation\n",
    "aggregated_table_part_1.createOrReplaceTempView(\"Combined_table\")\n",
    "\n",
    "#aggregating all tables as per the data model\n",
    "fully_combined_table = spark.sql('''SELECT product_id, product_name, t.aisle_id,aisle, d.department_id, department, o.order_id, user_id, \n",
    "                                    add_to_cart_order, reordered,eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order\n",
    "                                   FROM Combined_table t \n",
    "                                   INNER JOIN orders o ON o.order_id=t.order_id \n",
    "                                   INNER JOIN aisles a ON a.aisle_id=t.aisle_id\n",
    "                                   INNER JOIN department d ON d.department_id=t.department_id''')\n",
    "\n",
    "                \n",
    "#C Loading results to destination:- writing back tranformed data to destination (data lake):-\n",
    "#Note:- Here I have used coalesce so as to repartition the data to save it as a single file so as to make it easy to \n",
    "#use it for visualization part. However it is not a recommended step as repartition is a costly process.\n",
    "\n",
    "fully_combined_table.coalesce(1).write.option(\"header\",True).csv(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
