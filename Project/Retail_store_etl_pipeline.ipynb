{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up environement\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/share/spark-3.1.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "\n",
    "#defining sparksession\n",
    "spark = SparkSession.builder.appName('SparkSQL_UseCase').master('local[*]').getOrCreate()\n",
    "\n",
    "\n",
    "#Specifying File Schemas (i.e. Mentioning column name, datatype and Null value status of each column):-\n",
    "\n",
    "#1 Aisles Schema:-\n",
    "aisles_schema= StructType([StructField('aisle_id',IntegerType(),False),\n",
    "                          StructField('aisle',StringType(),True)])\n",
    "\n",
    "#2 Departments_schema:-\n",
    "department_schema=StructType([StructField('department_id',IntegerType(),False),\n",
    "                                StructField('department',StringType(),True)])\n",
    "#3 order_schema:-\n",
    "orders_schema=StructType([StructField('order_id',IntegerType(),False),\n",
    "                              StructField('user_id',IntegerType(),True),\n",
    "                              StructField('eval_set',StringType(),True),\n",
    "                              StructField('order_number',IntegerType(),True),\n",
    "                              StructField('order_dow',IntegerType(),True),\n",
    "                              StructField('order_hour_of_day',IntegerType(),True),\n",
    "                              StructField('days_since_prior_order',IntegerType(),True)])\n",
    "\n",
    "#4 prior_order_schema and train_order_schema:-\n",
    "prior_order_schema=StructType([StructField('order_id',IntegerType(),True),\n",
    "                              StructField('product_id',IntegerType(),True),\n",
    "                              StructField('add_to_cart_order',IntegerType(),True),\n",
    "                              StructField('reordered',IntegerType(),True)])\n",
    "#5 Products_schema:-\n",
    "products_schema=StructType([StructField('product_id',IntegerType(),False),\n",
    "                              StructField('product_name',StringType(),True),\n",
    "                              StructField('aisle_id',StringType(),True),\n",
    "                              StructField('department_id',StringType(),True)])\n",
    "\n",
    "# A) Extracting Data:-\n",
    "#defining file path from where to read the files and output path\n",
    "#Note:- The data has been copied to local and then given the local path here as I was facing issues with Insofe cluster for my IP.\n",
    "dataset_path='/home/fai10105/Project/Data_sets/'\n",
    "output_path=dataset_path+\"/output/\"\n",
    "\n",
    "#reading files as dataframes:-\n",
    "\n",
    "#aisles\n",
    "aisles_df = spark.read\\\n",
    "        .schema(aisles_schema)\\\n",
    "        .option(\"delimeter\",\",\").option(\"header\",\"True\")\\\n",
    "        .csv(dataset_path+'aisles.csv')\n",
    "\n",
    "#departments:-\n",
    "department_df = spark.read\\\n",
    "                .schema(department_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'departments.csv')\n",
    "#orders:-\n",
    "orders_df = spark.read\\\n",
    "                .schema(orders_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'orders.csv')\n",
    "\n",
    "#prior_order:-\n",
    "prior_order_df = spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'prior_order.csv')\n",
    "\n",
    "#products:- reading products file as rdd as it has some noises later on it has been converted to data frame after removing noises. \n",
    "#All other files have been read as csv\n",
    "products_rdd = spark.sparkContext\\\n",
    "                .textFile(dataset_path+'products.csv')\n",
    "\n",
    "#train_order:-\n",
    "train_order_df= spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'train_order.csv')\n",
    "\n",
    "\n",
    "#B) Transformation:- Data Processing Part\n",
    "\n",
    "#removing noises from products data:- removing unwanted characters from records like:- '\\' , '\"' , ',' etc\n",
    "def remove_noise(row):\n",
    "    if '\"' in row:\n",
    "        first=row.index('\"')\n",
    "        last=row.index('\"',first+1)\n",
    "        part_a=row[0:first]\n",
    "        part_b=row[first:last+1].replace(\", \",\" - \").replace('\"','')\n",
    "        part_c=row[last+1:]\n",
    "        row=(part_a+part_b+part_c).replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "    else:\n",
    "        row = row.replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "\n",
    "header=products_rdd.first()\n",
    "products_rdd_mo=products_rdd.filter(lambda x : x!=header).map(lambda x : remove_noise(x))\n",
    "products_df=products_rdd_mo.toDF(products_schema) # product dataframe creation from product rdd after removing noises.\n",
    "\n",
    "\n",
    "#Creating Tables from dataframes for aggregation purposes:-\n",
    "aisles_df.createOrReplaceTempView('aisles') # aisles table\n",
    "department_df.createOrReplaceTempView('department') # department table\n",
    "orders_df.createOrReplaceTempView('orders') # orders table\n",
    "prior_order_df.createOrReplaceTempView('prior_order') #prior_order table\n",
    "products_df.createOrReplaceTempView('products') #products table\n",
    "train_order_df.createOrReplaceTempView('train_order') #train_order table\n",
    "\n",
    "#aggregating products, prior_order and train_order data first (just to make the process easy abd simple)\n",
    "aggregated_table_part_1 =spark.sql('''SELECT p.product_id, product_name, aisle_id, department_id, order_id, add_to_cart_order, reordered\n",
    "                                      FROM products p INNER JOIN train_order to ON to.product_id=p.product_id\n",
    "                                      UNION ALL\n",
    "                                      SELECT p.product_id, product_name, aisle_id, department_id, order_id,add_to_cart_order,reordered\n",
    "                                      FROM products p INNER JOIN prior_order po ON po.product_id=p.product_id''')\n",
    "\n",
    "#creating table from aggregated_table_part_1 dataframe for further aggregation\n",
    "aggregated_table_part_1.createOrReplaceTempView(\"Combined_table\")\n",
    "\n",
    "#aggregating all tables as per the data model\n",
    "fully_combined_table = spark.sql('''SELECT product_id, product_name, t.aisle_id,aisle, d.department_id, department, o.order_id, user_id, \n",
    "                                    add_to_cart_order, reordered,eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order\n",
    "                                   FROM Combined_table t \n",
    "                                   INNER JOIN orders o ON o.order_id=t.order_id \n",
    "                                   INNER JOIN aisles a ON a.aisle_id=t.aisle_id\n",
    "                                   INNER JOIN department d ON d.department_id=t.department_id''')\n",
    "\n",
    "                \n",
    "#C Loading results to destination:- writing back tranformed data to destination (data lake):-\n",
    "#Note:- Here I have used coalesce so as to repartition the data to save it as a single file so as to make it easy to \n",
    "#use it for visualization part. However it is not a recommended step as repartition is a costly process.\n",
    "\n",
    "fully_combined_table.coalesce(1).write.option(\"header\",True).csv(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
