{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up environement and importing all required packages.\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/share/spark-3.1.1-bin-hadoop3.2\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining and configuring sparksession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Retail_store_Pipeline')\\\n",
    "        .master('local[*]')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done...!!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #Specifying File Schemas (i.e. Mentioning column name, datatype and Null value status of each column):-\n",
    "    #1 Aisles Schema:-\n",
    "    aisles_schema= StructType([StructField('aisle_id',IntegerType(),False),\n",
    "                              StructField('aisle',StringType(),True)])\n",
    "    #2 Departments_schema:-\n",
    "    department_schema=StructType([StructField('department_id',IntegerType(),False),\n",
    "                                    StructField('department',StringType(),True)])\n",
    "    #3 order_schema:-\n",
    "    orders_schema=StructType([StructField('order_id',IntegerType(),False),\n",
    "                                  StructField('user_id',IntegerType(),True),\n",
    "                                  StructField('eval_set',StringType(),True),\n",
    "                                  StructField('order_number',IntegerType(),True),\n",
    "                                  StructField('order_dow',IntegerType(),True),\n",
    "                                  StructField('order_hour_of_day',IntegerType(),True),\n",
    "                                  StructField('days_since_prior_order',IntegerType(),True)])\n",
    "    #4 prior_order_schema and train_order_schema:-\n",
    "    prior_order_schema=StructType([StructField('order_id',IntegerType(),True),\n",
    "                                  StructField('product_id',IntegerType(),True),\n",
    "                                  StructField('add_to_cart_order',IntegerType(),True),\n",
    "                                  StructField('reordered',IntegerType(),True)])\n",
    "    #5 Products_schema:-\n",
    "    products_schema=StructType([StructField('product_id',IntegerType(),False),\n",
    "                                  StructField('product_name',StringType(),True),\n",
    "                                  StructField('aisle_id',StringType(),True),\n",
    "                                  StructField('department_id',StringType(),True)])\n",
    "except:\n",
    "    print('error occoured..!!')\n",
    "finally:\n",
    "    print(\"All done...!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''defining source and destination data lake path as variables.\n",
    "Note:- The data has been copied to local and then given the local path here as \n",
    "I was facing issues with Insofe cluster for my IP.'''\n",
    "\n",
    "dataset_path='/home/fai10105/Project/Data_sets/' #Data source location\n",
    "output_path=dataset_path+\"/output/\"              #Output location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done...!!\n"
     ]
    }
   ],
   "source": [
    "'''A) Extracting Data:-reading files as dataframes except products:-'''\n",
    "try:\n",
    "    #aisles\n",
    "    aisles_df = spark.read.schema(aisles_schema)\\\n",
    "                    .option(\"delimeter\",\",\").option(\"header\",\"True\")\\\n",
    "                    .csv(dataset_path+'aisles.csv')\n",
    "    #departments:-\n",
    "    department_df = spark.read.schema(department_schema)\\\n",
    "                    .option(\"header\",\"True\")\\\n",
    "                    .csv(dataset_path+'departments.csv')\n",
    "    #orders:-\n",
    "    orders_df = spark.read.schema(orders_schema)\\\n",
    "                    .option(\"header\",\"True\")\\\n",
    "                    .csv(dataset_path+'orders.csv')\n",
    "    #prior_order:-\n",
    "    prior_order_df = spark.read\\\n",
    "                    .schema(prior_order_schema)\\\n",
    "                    .option(\"header\",\"True\")\\\n",
    "                    .csv(dataset_path+'prior_order.csv')\n",
    "    #train_order:-\n",
    "    train_order_df= spark.read\\\n",
    "                    .schema(prior_order_schema)\\\n",
    "                    .option(\"header\",\"True\")\\\n",
    "                    .csv(dataset_path+'train_order.csv')\n",
    "except:\n",
    "    print('error occoured..!')\n",
    "finally:\n",
    "    print('All done...!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all fine\n"
     ]
    }
   ],
   "source": [
    "'''products:- reading products file as rdd first as it has some noises later on \n",
    "it has been converted to data frame after removing noises. \n",
    "All other files have been read as data frames'''\n",
    "try:\n",
    "    products_rdd = spark.sparkContext\\\n",
    "                    .textFile(dataset_path+'products.csv')\n",
    "\n",
    "    #removing noises from products data:- removing unwanted characters from records like:- '\\' , '\"' , ',' etc\n",
    "    #after removing noises we will convert it to dataframe\n",
    "    def remove_noise(row):\n",
    "        if '\"' in row:\n",
    "            first=row.index('\"')\n",
    "            last=row.index('\"',first+1)\n",
    "            part_a=row[0:first]\n",
    "            part_b=row[first:last+1].replace(\", \",\" - \").replace('\"','')\n",
    "            part_c=row[last+1:]\n",
    "            row=(part_a+part_b+part_c).replace('\\\"',\"\").split(\",\")\n",
    "            return [int(row[0]),row[1],row[2],row[3]]\n",
    "        else:\n",
    "            row = row.replace('\\\"',\"\").split(\",\")\n",
    "            return [int(row[0]),row[1],row[2],row[3]]\n",
    "\n",
    "    header=products_rdd.first()\n",
    "    products_rdd_mo=products_rdd.filter(lambda x : x!=header).map(lambda x : remove_noise(x))\n",
    "    products_df=products_rdd_mo.toDF(products_schema) # product dataframe creation from product rdd after removing noises.\n",
    "except:\n",
    "    print('error occoured..!!')\n",
    "finally:\n",
    "    print('all fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary which will store values in form of (table_name,dataframe_name) as (key,value)\n",
    "all_data_frames={'Aisles': aisles_df, 'Department':department_df,'Products':products_df,\n",
    "                 'Orders':orders_df,'Prior_order':prior_order_df,'Train_order':train_order_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done..!!\n"
     ]
    }
   ],
   "source": [
    "#Function for Displaying the columns names in each data frames\n",
    "try:\n",
    "    def checking_column_names(all_data_frames):\n",
    "        print('Column names in all the tables are as follows:\\n')\n",
    "        for table_name,df_name in all_data_frames.items():\n",
    "            print(table_name,':-')\n",
    "            print(df_name.columns,\"\\n\")\n",
    "except:\n",
    "    print('error occoured..!!')\n",
    "finally:\n",
    "    print('All Done..!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in all the tables are as follows:\n",
      "\n",
      "Aisles :-\n",
      "['aisle_id', 'aisle'] \n",
      "\n",
      "Department :-\n",
      "['department_id', 'department'] \n",
      "\n",
      "Products :-\n",
      "['product_id', 'product_name', 'aisle_id', 'department_id'] \n",
      "\n",
      "Orders :-\n",
      "['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order'] \n",
      "\n",
      "Prior_order :-\n",
      "['order_id', 'product_id', 'add_to_cart_order', 'reordered'] \n",
      "\n",
      "Train_order :-\n",
      "['order_id', 'product_id', 'add_to_cart_order', 'reordered'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the columns names in each data frames    \n",
    "checking_column_names(all_data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Done..!!\n"
     ]
    }
   ],
   "source": [
    "#Display the datatypes of the columns in each data frames\n",
    "try:\n",
    "    def checking_data_types(all_data_frames):\n",
    "        print('datatypes of Column names in all the tables are as follows:\\n')\n",
    "        for table_name,df_name in all_data_frames.items():\n",
    "            print(table_name,':-')\n",
    "            df_name.printSchema()\n",
    "            print()\n",
    "except:\n",
    "    print('error occoured..!!')\n",
    "finally:\n",
    "    print('All Done..!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatypes of Column names in all the tables are as follows:\n",
      "\n",
      "Aisles :-\n",
      "root\n",
      " |-- aisle_id: integer (nullable = true)\n",
      " |-- aisle: string (nullable = true)\n",
      "\n",
      "\n",
      "Department :-\n",
      "root\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "\n",
      "Products :-\n",
      "root\n",
      " |-- product_id: integer (nullable = false)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- aisle_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      "\n",
      "\n",
      "Orders :-\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- eval_set: string (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: integer (nullable = true)\n",
      "\n",
      "\n",
      "Prior_order :-\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n",
      "\n",
      "Train_order :-\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying data types using the function created\n",
    "checking_data_types(all_data_frames)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done...!!\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in the columns:-\n",
    "try:\n",
    "    def check_for_null(all_data_frames):\n",
    "        print('Count of Null values in each Column in all the tables is as follows:\\n')\n",
    "        for table_name,df_name in all_data_frames.items():\n",
    "            print(table_name,':-')\n",
    "            print({col: df_name.filter(df_name[col].isNull()).count() for col in df_name.columns},'\\n')\n",
    "except:\n",
    "    print(\"Error occoured\")\n",
    "finally:\n",
    "    print(\"All done...!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Null values in each Column in all the tables is as follows:\n",
      "\n",
      "Aisles :-\n",
      "{'aisle_id': 0, 'aisle': 0} \n",
      "\n",
      "Department :-\n",
      "{'department_id': 0, 'department': 0} \n",
      "\n",
      "Products :-\n",
      "{'product_id': 0, 'product_name': 0, 'aisle_id': 0, 'department_id': 0} \n",
      "\n",
      "Orders :-\n",
      "{'order_id': 0, 'user_id': 0, 'eval_set': 0, 'order_number': 0, 'order_dow': 0, 'order_hour_of_day': 0, 'days_since_prior_order': 82683} \n",
      "\n",
      "Prior_order :-\n",
      "{'order_id': 0, 'product_id': 0, 'add_to_cart_order': 0, 'reordered': 0} \n",
      "\n",
      "Train_order :-\n",
      "{'order_id': 0, 'product_id': 0, 'add_to_cart_order': 0, 'reordered': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying Null values usimg the function created\n",
    "check_for_null(all_data_frames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done...!!\n"
     ]
    }
   ],
   "source": [
    "#B) Transformation:- Data Processing Part\n",
    "try:\n",
    "    #Creating Tables from dataframes for aggregation purposes:-\n",
    "    aisles_df.createOrReplaceTempView('aisles') # aisles table\n",
    "    department_df.createOrReplaceTempView('department') # department table\n",
    "    orders_df.createOrReplaceTempView('orders') # orders table\n",
    "    prior_order_df.createOrReplaceTempView('prior_order') #prior_order table\n",
    "    products_df.createOrReplaceTempView('products') #products table\n",
    "    train_order_df.createOrReplaceTempView('train_order') #train_order table\n",
    "except:\n",
    "    print(\"Error occoured\")\n",
    "finally:\n",
    "    print(\"All done...!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|aisle_id|               aisle|\n",
      "+--------+--------------------+\n",
      "|       1|prepared soups sa...|\n",
      "|       2|   specialty cheeses|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from aisles\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|department_id|department|\n",
      "+-------------+----------+\n",
      "|            1|    frozen|\n",
      "|            2|     other|\n",
      "+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from department\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 1363380|     50|   prior|           1|        3|                9|                  null|\n",
      "| 3131103|     50|   prior|           2|        6|               12|                  null|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from orders\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|      12|     30597|                1|        1|\n",
      "|      12|     15221|                2|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from prior_order\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|    1077|     13176|                1|        1|\n",
      "|    1077|     39922|                2|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from train_order\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from products\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+-------------+--------+-----------------+---------+\n",
      "|product_id|        product_name|aisle_id|department_id|order_id|add_to_cart_order|reordered|\n",
      "+----------+--------------------+--------+-------------+--------+-----------------+---------+\n",
      "|         1|Chocolate Sandwic...|      61|           19| 1290664|                3|        0|\n",
      "|         2|    All-Seasons Salt|     104|           13| 1455635|               26|        0|\n",
      "+----------+--------------------+--------+-------------+--------+-----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregating products, prior_order and train_order data first (just to make the process easy abd optimized)\n",
    "aggregated_table_part_1 =spark.sql('''SELECT p.product_id, product_name, aisle_id, department_id, order_id, add_to_cart_order, \n",
    "                                      reordered FROM products p INNER JOIN train_order to ON to.product_id=p.product_id\n",
    "                                      UNION ALL\n",
    "                                      SELECT p.product_id, product_name, aisle_id, department_id, order_id,add_to_cart_order,\n",
    "                                      reordered FROM products p INNER JOIN prior_order po ON po.product_id=p.product_id''')\n",
    "\n",
    "#creating table from aggregated_table_part_1 dataframe for further aggregation\n",
    "aggregated_table_part_1.createOrReplaceTempView(\"Combined_table\")\n",
    "\n",
    "spark.sql(\"Select * from combined_table\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-----------------+\n",
      "|product_id|order_id|user_id|add_to_cart_order|\n",
      "+----------+--------+-------+-----------------+\n",
      "|         1| 1290664|  79605|                3|\n",
      "|         2| 1455635|  27086|               26|\n",
      "|         3| 2188727|  83549|                1|\n",
      "|        10| 2180721| 146967|                4|\n",
      "|        10| 1965683| 205353|               13|\n",
      "+----------+--------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregating all tables as per the data model\n",
    "fully_combined_table = spark.sql('''SELECT product_id, product_name, t.aisle_id,aisle, d.department_id, department, \n",
    "                                    o.order_id, user_id,add_to_cart_order, reordered,eval_set, order_number, order_dow, \n",
    "                                    order_hour_of_day, days_since_prior_order\n",
    "                                    FROM Combined_table t \n",
    "                                    INNER JOIN orders o ON o.order_id=t.order_id \n",
    "                                    INNER JOIN aisles a ON a.aisle_id=t.aisle_id\n",
    "                                    INNER JOIN department d ON d.department_id=t.department_id''')\n",
    "\n",
    "fully_combined_table.select(\"product_id\",\"order_id\", \"user_id\",\"add_to_cart_order\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "                \n",
    "#C Loading results to destination:- writing back tranformed data to destination (data lake):-\n",
    "#Note:- Here I have used coalesce so as to repartition the data to save it as a single file so as to make it easy to \n",
    "#use it for visualization part. However it is not a recommended step as repartition is a costly process.\n",
    "\n",
    "fully_combined_table.coalesce(1).write.option(\"header\",True).csv(output_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
